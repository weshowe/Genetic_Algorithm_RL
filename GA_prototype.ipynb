{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\icarus\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gym\\wrappers\\monitoring\\video_recorder.py:9: DeprecationWarning: The distutils package is deprecated and slated for removal in Python 3.12. Use setuptools or check PEP 632 for potential alternatives\n",
      "  import distutils.spawn\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Dependencies: gym, ale-py\n",
    "\n",
    "pip install gym[atari,accept-rom-license]\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from copy import copy, deepcopy\n",
    "\n",
    "from functools import cmp_to_key\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Action space is of size 7:\n",
    "\n",
    "0 NOOP\n",
    "1 FIRE\n",
    "2 UP\n",
    "3 RIGHT\n",
    "4 LEFT\n",
    "5 RIGHTFIRE\n",
    "6 LEFTFIRE\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "A class for each chromosome.\n",
    "\"\"\"\n",
    "class Chromosome:\n",
    "\n",
    "    \"\"\"\n",
    "    This is used to initialize a chromosome\n",
    "\n",
    "    args is used since Python doesn't support multiple constructors. (self, env) initializes a chromosome with a random\n",
    "    action sequence and is used during population initialization.\n",
    "\n",
    "    (self, actions, env) is used during crossover to generate new chromosomes given a crossed action buffer.\n",
    "    \"\"\"\n",
    "    def __init__(self, *args):\n",
    "\n",
    "        if len(args) > 1:\n",
    "            self.action_buffer = args[0]\n",
    "            self.env = args[1]\n",
    "            self.reward_sum = 0\n",
    "            self.time_alive = 0\n",
    "            return\n",
    "\n",
    "        # Stores actions taken, rewards obtained, total reward for the run, and the number of frames that the agent has been alive for.\n",
    "        self.action_buffer = []\n",
    "        self.reward_buffer = []\n",
    "\n",
    "        self.reward_sum = 0\n",
    "        self.time_alive = 0\n",
    "\n",
    "        # Store the provided env as a class variable.\n",
    "        self.env = args[0]\n",
    "\n",
    "        _ = self.env.reset()\n",
    "        done = False\n",
    "\n",
    "        num_lives = 4\n",
    "\n",
    "        while(done == False):\n",
    "    \n",
    "            action = random.randint(0, 6)\n",
    "\n",
    "            _, reward, done, info = self.env.step(action)\n",
    "\n",
    "            self.action_buffer.append(action)\n",
    "            self.reward_buffer.append(reward)\n",
    "\n",
    "            self.time_alive = self.time_alive + 1\n",
    "            self.reward_sum = self.reward_sum + reward\n",
    "\n",
    "            if info['lives'] > num_lives:\n",
    "                num_lives = info['lives']\n",
    "\n",
    "            if info['lives'] < num_lives:\n",
    "                break\n",
    "\n",
    "    \"\"\"\n",
    "    This reruns the environment and performs a new run with the updated action buffer to generate new stats.\n",
    "    \"\"\"\n",
    "    def update(self):\n",
    "\n",
    "        # Stores updated actions and rewards taken.\n",
    "        new_action_buffer = []\n",
    "        new_reward_buffer = []\n",
    "\n",
    "        # This is needed in case we manage to survive for more frames than the action buffer has actions, in which case we do random ones.\n",
    "        limit = self.time_alive\n",
    "        counter = 0\n",
    "\n",
    "        self.reward_sum = 0\n",
    "        self.time_alive = 0\n",
    "\n",
    "        _ = self.env.reset()\n",
    "\n",
    "        done = False\n",
    "\n",
    "        num_lives = 4\n",
    "\n",
    "        while(done == False):\n",
    "\n",
    "            # Determine if action is from supplied buffer or random.\n",
    "            counter = counter + 1\n",
    "\n",
    "            if counter <= limit:\n",
    "                action = self.action_buffer[counter-1]\n",
    "            else:  \n",
    "                action = random.randint(0, 6)\n",
    "\n",
    "            _, reward, done, info = self.env.step(action)\n",
    "\n",
    "            new_action_buffer.append(action)\n",
    "            new_reward_buffer.append(reward)\n",
    "            self.time_alive = self.time_alive + 1\n",
    "            self.reward_sum = self.reward_sum + reward\n",
    "\n",
    "            if info['lives'] > num_lives:\n",
    "                num_lives = info['lives']\n",
    "\n",
    "            if info['lives'] < num_lives:\n",
    "                break\n",
    "\n",
    "        self.action_buffer = new_action_buffer\n",
    "        self.reward_buffer = new_reward_buffer\n",
    "\n",
    "    \"\"\"\n",
    "    Since the environment has a certain level of randomness, this function reruns it without saving results\n",
    "    except the aggregate reward and time alive. Used to gauge how accurate a score is during fitness.\n",
    "    \"\"\"\n",
    "    def simulate(self):\n",
    "\n",
    "        # This is needed in case we manage to survive for more frames than the action buffer has actions, in which case we do random ones.\n",
    "        limit = self.time_alive\n",
    "        counter = 0\n",
    "\n",
    "        new_reward_sum = 0\n",
    "        new_time_alive = 0\n",
    "\n",
    "        _ = self.env.reset()\n",
    "\n",
    "        done = False\n",
    "\n",
    "        num_lives = 4\n",
    "\n",
    "        while(done == False):\n",
    "\n",
    "            # Determine if action is from supplied buffer or random.\n",
    "            counter = counter + 1\n",
    "\n",
    "            if counter <= limit:\n",
    "                action = self.action_buffer[counter-1]\n",
    "            else:  \n",
    "                action = random.randint(0, 6)\n",
    "\n",
    "            _, reward, done, info = self.env.step(action)\n",
    "\n",
    "            new_time_alive = self.time_alive + 1\n",
    "            new_reward_sum = self.reward_sum + reward\n",
    "\n",
    "            if info['lives'] > num_lives:\n",
    "                num_lives = info['lives']\n",
    "\n",
    "            if info['lives'] < num_lives:\n",
    "                break\n",
    "\n",
    "        return new_reward_sum, new_time_alive\n",
    "\n",
    "    \"\"\"\n",
    "    This mutates a chromosome's genes (individual actions taken) based on a supplied probability. If the mutation occurs, the action is replaced with a random one.\n",
    "    \"\"\"\n",
    "    def mutate(self, mutation_rate):\n",
    "        for i in range(0, len(self.action_buffer)):\n",
    "            if random.random() <= mutation_rate:\n",
    "                self.action_buffer[i] = random.randint(0,6)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "GA and helper methods\n",
    "\"\"\"\n",
    "\n",
    "#Comparator for a Chromosome list\n",
    "def chromosome_comparator(a, b):\n",
    "    if a.reward_sum > b.reward_sum:\n",
    "        return -1\n",
    "    elif a.reward_sum == b.reward_sum:\n",
    "        if a.time_alive < b.time_alive:\n",
    "            return -1\n",
    "        else:\n",
    "            return 1\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "# Feed this to sorting function\n",
    "compare_key = cmp_to_key(chromosome_comparator)\n",
    "\n",
    "# Generates initial population\n",
    "def create_initial_population(number, env):\n",
    "    population = []\n",
    "\n",
    "    for i in range(0, number):\n",
    "        population.append(Chromosome(env))\n",
    "\n",
    "    return population\n",
    "\n",
    "\"\"\"\n",
    "Applies a fitness function and then ranks the population in descending order of best to worst.\n",
    "\n",
    "In this case, the primary marker of fitness is the high score. Assuming a tie, then secondary criterion is how long each agent has been alive.\n",
    "\n",
    "Also returns the best chromosome found.\n",
    "\"\"\"\n",
    "def selection(population):\n",
    "\n",
    "    # The first stage is the fitness function.\n",
    "    optimal_index = 0\n",
    "    longest_alive = 1000000\n",
    "    max_score = 0\n",
    "\n",
    "    for i, chrom in enumerate(population):\n",
    "\n",
    "        total_reward = []\n",
    "        total_time = []\n",
    "\n",
    "        # Since the game isn't purely deterministic (some variation in how a run plays out), we average the results of a given action list.\n",
    "        for j in range(0, 10):\n",
    "            current_reward, current_time = chrom.simulate()\n",
    "            total_reward.append(current_reward)\n",
    "            total_time.append(current_time)\n",
    "\n",
    "        mean_reward = np.mean(np.array(total_reward))\n",
    "        mean_time = np.mean(np.array(total_time))\n",
    "\n",
    "        # Update the chromosome's measure of its fitness with the mean reward.\n",
    "        chrom.reward_sum = mean_reward\n",
    "\n",
    "        if mean_reward > max_score:\n",
    "            best = True\n",
    "        elif (mean_reward == max_score) and mean_time < longest_alive:\n",
    "            best = True\n",
    "        else:\n",
    "            best = False\n",
    "\n",
    "        if best:\n",
    "            longest_alive = mean_time\n",
    "            max_score = mean_reward\n",
    "            optimal_index = i\n",
    "\n",
    "    # Select best chromosome.\n",
    "    best_chromosome = population[optimal_index]\n",
    "\n",
    "    # Sort chromosomes in descending order.\n",
    "    population.sort(key=compare_key, reverse=True)\n",
    "\n",
    "    # This was for an AIS test, should remain commented out.\n",
    "\n",
    "    #for i in range(0, len(population)):\n",
    "        #population[i] = best_chromosome\n",
    "\n",
    "    # Select only the best 50% of chromosomes.\n",
    "    return population[0:int(len(population)/2)], best_chromosome\n",
    "\n",
    "\"\"\"\n",
    "Performs crossover on a population and returns the new population.\n",
    "\n",
    "Crossover is done by selecting the actions that led to the highest rewards for each given action, reward pair for both parents.\n",
    "\n",
    "Population is split into pairs of parents and crossover is performed, yielding 4 children in each instance.\n",
    "\"\"\"\n",
    "def crossover(population):\n",
    "    crossed_population = []\n",
    "    for i in range(0, len(population) - 1, 2):\n",
    "\n",
    "        father = population[i]\n",
    "\n",
    "        # Edge case if there's only 1 chromosome left, in which case it pairs with a random one.\n",
    "        if i + 1 > len(population) - 1:\n",
    "            mother = population[random.randint(0, len(population)-1)]\n",
    "            \n",
    "        else:\n",
    "            mother = population[i+1]\n",
    "\n",
    "        limit = 0\n",
    "\n",
    "        if father.time_alive < mother.time_alive:\n",
    "            new_actions = deepcopy(father.action_buffer)\n",
    "            limit = mother.time_alive - 1\n",
    "            \n",
    "        else:\n",
    "            new_actions = deepcopy(mother.action_buffer)\n",
    "            limit = father.time_alive - 1\n",
    "\n",
    "        for j in range(0, len(new_actions)):\n",
    "\n",
    "            # We can't permute actions once we've reached a point where we've exhausted the buffer of one parent.\n",
    "            if j > limit:\n",
    "                break\n",
    "\n",
    "            if mother.reward_buffer[j] > father.reward_buffer[j]:\n",
    "                new_actions[j] = mother.action_buffer[j]\n",
    "\n",
    "            elif father.reward_buffer[j] > mother.reward_buffer[j]:\n",
    "                new_actions[j] = father.action_buffer[j]\n",
    "\n",
    "        # They each have 4 kids, replenishing the population.\n",
    "        crossed_population.append(Chromosome(new_actions, father.env))\n",
    "        crossed_population.append(Chromosome(new_actions, father.env))\n",
    "        crossed_population.append(Chromosome(new_actions, father.env))\n",
    "        crossed_population.append(Chromosome(new_actions, father.env))\n",
    "\n",
    "    return crossed_population\n",
    "\n",
    "\"\"\"\n",
    "Mutate the population. Rate represents chance of mutation.\n",
    "\"\"\"\n",
    "def mutate_population(population, rate=0.01):\n",
    "    for _, chrom in enumerate(population): \n",
    "        chrom.mutate(rate)\n",
    "\n",
    "\"\"\"\n",
    "Rerun the environment for all chromosomes and produce updated runs.\n",
    "\"\"\"\n",
    "def update_population(population):\n",
    "    for i, chrom in enumerate(population):     \n",
    "         chrom.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Chromosome for Run 1 -> Longest Alive: 183    High Score: 105.0\n",
      "Best Chromosome for Run 2 -> Longest Alive: 182    High Score: 126.0\n",
      "Best Chromosome for Run 3 -> Longest Alive: 167    High Score: 126.0\n",
      "Best Chromosome for Run 4 -> Longest Alive: 182    High Score: 126.0\n",
      "Best Chromosome for Run 5 -> Longest Alive: 182    High Score: 168.0\n",
      "Best Chromosome for Run 6 -> Longest Alive: 215    High Score: 126.0\n",
      "Best Chromosome for Run 7 -> Longest Alive: 215    High Score: 168.0\n",
      "Best Chromosome for Run 8 -> Longest Alive: 129    High Score: 126.0\n",
      "Best Chromosome for Run 9 -> Longest Alive: 151    High Score: 126.0\n",
      "Best Chromosome for Run 10 -> Longest Alive: 198    High Score: 147.0\n",
      "Best Chromosome -> Longest Alive: 182    High Score: 168.0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "GA loop\n",
    "\"\"\"\n",
    "\n",
    "env = gym.make('ALE/Assault-v5', full_action_space=False)\n",
    "\n",
    "n_iter = 10\n",
    "population_size = 10\n",
    "mutation_rate = 0.02\n",
    "\n",
    "best_alive = 0\n",
    "best_score = 0\n",
    "best_chromo = None\n",
    "\n",
    "population = create_initial_population(population_size, env)\n",
    "\n",
    "for i in range(0, n_iter):\n",
    "\n",
    "    # Run the fitness function and determine if the best chromosome is better than the best one we've stored for all runs.\n",
    "    selected_population, best_chromo_candidate = selection(population)\n",
    "    longest_alive = best_chromo_candidate.time_alive\n",
    "    max_score = best_chromo_candidate.reward_sum\n",
    "\n",
    "    if best_chromo_candidate.reward_sum > best_score:\n",
    "            best = True\n",
    "    elif (best_chromo_candidate.reward_sum == best_score) and best_chromo_candidate.time_alive < best_alive:\n",
    "            best = True\n",
    "    else:\n",
    "        if i == 0:\n",
    "                best = True\n",
    "        else:\n",
    "                best = False\n",
    "\n",
    "    if best:\n",
    "           best_chromo = deepcopy(best_chromo_candidate)\n",
    "           best_alive = best_chromo_candidate.time_alive\n",
    "           best_score = best_chromo_candidate.reward_sum\n",
    "           \n",
    "\n",
    "    # Print stats.\n",
    "    print(f\"Best Chromosome for Run {i + 1} -> Longest Alive: {longest_alive}    High Score: {max_score}\")\n",
    "\n",
    "    # Cross, Mutate, Update, and prepare the new population for the next run.\n",
    "    crossed_population = crossover(selected_population)\n",
    "    mutate_population(crossed_population, mutation_rate)\n",
    "    update_population(crossed_population)\n",
    "\n",
    "    population = crossed_population\n",
    "\n",
    "print(f\"Best Chromosome -> Longest Alive: {best_alive}    High Score: {best_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\icarus\\Documents\\school\\Winter 2023\\MAT 5182\\project\\Genetic_Algorithm_RL\\GA_prototype.ipynb Cell 5\u001b[0m in \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/icarus/Documents/school/Winter%202023/MAT%205182/project/Genetic_Algorithm_RL/GA_prototype.ipynb#W4sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39melse\u001b[39;00m:  \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/icarus/Documents/school/Winter%202023/MAT%205182/project/Genetic_Algorithm_RL/GA_prototype.ipynb#W4sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     action \u001b[39m=\u001b[39m random\u001b[39m.\u001b[39mrandint(\u001b[39m0\u001b[39m, \u001b[39m6\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/icarus/Documents/school/Winter%202023/MAT%205182/project/Genetic_Algorithm_RL/GA_prototype.ipynb#W4sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m _, reward, done, info \u001b[39m=\u001b[39m test_env\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/icarus/Documents/school/Winter%202023/MAT%205182/project/Genetic_Algorithm_RL/GA_prototype.ipynb#W4sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m reward_sum \u001b[39m=\u001b[39m reward_sum \u001b[39m+\u001b[39m reward\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/icarus/Documents/school/Winter%202023/MAT%205182/project/Genetic_Algorithm_RL/GA_prototype.ipynb#W4sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m time_alive \u001b[39m=\u001b[39m time_alive \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\icarus\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gym\\wrappers\\time_limit.py:17\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[1;32m---> 17\u001b[0m     observation, reward, done, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m     18\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     19\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[1;32mc:\\Users\\icarus\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gym\\wrappers\\order_enforcing.py:13\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[0;32m     12\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_reset, \u001b[39m\"\u001b[39m\u001b[39mCannot call env.step() before calling reset()\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m---> 13\u001b[0m     observation, reward, done, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m     14\u001b[0m     \u001b[39mreturn\u001b[39;00m observation, reward, done, info\n",
      "File \u001b[1;32mc:\\Users\\icarus\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gym\\envs\\atari\\environment.py:238\u001b[0m, in \u001b[0;36mAtariEnv.step\u001b[1;34m(self, action_ind)\u001b[0m\n\u001b[0;32m    236\u001b[0m reward \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[0;32m    237\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(frameskip):\n\u001b[1;32m--> 238\u001b[0m     reward \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39male\u001b[39m.\u001b[39mact(action)\n\u001b[0;32m    240\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_obs(), reward, terminal, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_info()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This displays the game to evaluate our best chromosome.\n",
    "\"\"\"\n",
    "test_env = gym.make('ALE/Assault-v5', render_mode=\"human\", full_action_space=False)\n",
    "\n",
    "done = False\n",
    "_ = test_env.reset()\n",
    "\n",
    "time_alive = 0\n",
    "reward_sum = 0\n",
    "\n",
    "action_list = best_chromo.action_buffer\n",
    "limit = best_chromo.time_alive \n",
    "counter = 0\n",
    "\n",
    "while(done == False):\n",
    "\n",
    "    # Determine if action is from supplied buffer or random.\n",
    "    counter = counter + 1\n",
    "\n",
    "    if counter <= limit:\n",
    "        action = action_list[counter-1]\n",
    "    else:  \n",
    "        action = random.randint(0, 6)\n",
    "    \n",
    "    _, reward, done, info = test_env.step(action)\n",
    "\n",
    "    reward_sum = reward_sum + reward\n",
    "    time_alive = time_alive + 1\n",
    "\n",
    "print(f\"Longest Alive: {time_alive}    High Score: {reward_sum}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
